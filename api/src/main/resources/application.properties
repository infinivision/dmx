server.port=8080

spring.datasource.url = jdbc:mysql://172.19.0.101:3306/db_dmx_stage?characterEncoding=utf-8&autoReconnect=true&failOverReadOnly=false
spring.datasource.username = root
spring.datasource.password = root
spring.datasource.driver-class-name = com.mysql.cj.jdbc.Driver

spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
spring.jpa.hibernate.ddl-auto=update

spring.datasource.type = com.alibaba.druid.pool.DruidDataSource
spring.datasource.initialSize = 60
spring.datasource.minIdle = 1
spring.datasource.maxActive = 100
spring.datasource.maxWait = 6000
spring.datasource.timeBetweenEvictionRunsMillis = 100
spring.datasource.minEvictableIdleTimeMillis = 3000
spring.datasource.validationQuery = "SELECT 1 FROM DUAL"
spring.datasource.testWhileIdle = true
spring.datasource.testOnBorrow = false
spring.datasource.testOnReturn = false
spring.datasource.poolPreparedStatements = false
spring.datasource.maxPoolPreparedStatementPerConnectionSize = 20
spring.datasource.connectionProperties = "druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000"
spring.datasource.useGlobalDataSourceStat = true

# analysis configure
analysis.datasource.url = jdbc:postgresql://172.19.0.106:5432/db_dmx_stage
analysis.datasource.username = gpadmin
analysis.datasource.password = Welcome123@
analysis.datasource.driver-class-name = org.postgresql.Driver
analysis.jpa.properties.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect
analysis.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults = false
analysis.jpa.hibernate.ddl-auto=update


application.segment.size=100
application.tag.size=100

# standalone,yarn,yarnPer,local
flink.cluster.mode=standalone
flink.cluster.conf=/Users/zhouchangyue/Downloads/flink
flink.cluster.yarn.conf=/Users/zhouchangyue/Downloads/flink

flink.job.name.prefix=tag_calculate_group
flink.job.group.num=1
flink.job.jar.file=core.jar
flink.job.sql.path=/Users/zhouchangyue/Downloads/flink/plugins
flink.job.local.sql.plugins=/Users/zhouchangyue/Downloads/flink/plugins
flink.job.remote.sql.plugins=/opt/plugins
flink.job.conf.prop={"time.characteristic": "EventTime","sql.checkpoint.interval": 10000}
flink.job.save.path=
flink.job.save.allow_non_restore=true
flink.job.parallel.num=1


flink.streaming.source.table.desc = CREATE TABLE kafka_source(id int,name varchar,updateTime bigint,createTime bigint)WITH(type ='kafka11',bootstrapServers ='172.19.0.108:9092',zookeeperQuorum = '172.19.0.108:2181/kafka',offsetReset ='earliest',topic ='test',parallelism ='1')
flink.streaming.source.table.name = kafka_source
flink.streaming.dest.table.desc = CREATE TABLE mysql_sink(id int,name varchar,updateTime bigint,createTime bigint)WITH(type ='mysql',url ='jdbc:mysql://172.19.0.15:13306/flink_source?charset=utf8',userName ='root',password ='flink',tableName ='flink_test_14',parallelism ='1')
flink.streaming.dest.table.name = mysql_sink
flink.streaming.dest.table.columns= id,name,updateTime,createTime

#logging.level.org.hibernate.SQL=DEBUG
#logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE